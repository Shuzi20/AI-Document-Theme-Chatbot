
# ğŸ“š Document Theme Chatbot â€“ Wasserstoff Gen-AI Internship Task

This project is a **Document Research and Theme Identification Chatbot** built as part of the **Wasserstoff Generative AI Internship Qualification Task**. It enables users to upload a wide range of documents (PDFs, text, scanned images), ask natural language questions, and receive document-cited answers and theme-based summaries using LLMs and vector search.

---

## ğŸ” Features

### âœ… Core Functionality

- Upload **75+ documents** (PDF, scans, text)
- **OCR support** for scanned PDFs using PyMuPDF
- Chunking + embedding using **LangChain + MiniLM**
- Vector storage in **Qdrant**
- **Semantic search** to retrieve top matching document chunks
- Answers include:
  - **Document ID**
  - **Page number**
  - **Chunk number**
- Theme-based summarization using **Groq + LLaMA 3**
- Advanced filtering:
  - **By date**
  - **By document type**
  - **By inclusion/exclusion**
  - **Sort by relevance/newest/oldest**

---

### ğŸ¯ Extra Credit Implemented

- âœ… **Paragraph/Chunk-level citations** (e.g., *Page 3, Chunk 2*)
- âœ… **Clickable citations** inside chat that show chunk modal
- âœ… **Filter sidebar** shows only matched documents
- âœ… **Re-run** query on selected subset of docs
- âœ… Sidebar filters: exclude, sort, type, date
- âœ… Modern UI features:
  - Upload & loading states
  - **Clear Chat** option
  - **Chat history persisted** in localStorage
  - Chat-style flow with **latest message near input**, like ChatGPT

---

## ğŸ§  Tech Stack

| Layer     | Tooling                       |
|-----------|-------------------------------|
| Frontend  | Next.js 14, Tailwind CSS      |
| Backend   | FastAPI                       |
| LLM       | Groq API + LLaMA 3 (8B)       |
| Embedding | HuggingFace `all-MiniLM-L6-v2`|
| Vector DB | Qdrant                        |
| OCR       | PyMuPDF (fitz)                |
| Deployment| (Local Dev) â€“ HuggingFace, Render, Railway supported |

---

## ğŸ§ª How It Works

### ğŸ”¹ Document Upload
- Extract text from each page (OCR if needed)
- Split into overlapping chunks
- Embed using `MiniLM`
- Store in Qdrant with metadata:
  - doc_name, page, chunk_index, uploaded_at

### ğŸ”¹ Asking a Question
- Embed user question
- Search top-k relevant chunks in Qdrant
- Return each with doc ID, page, chunk
- Generate LLM summary using Groq LLaMA-3

### ğŸ”¹ Frontend Display
- Shows document answer table
- Synthesized theme summary
- Clickable references to exact chunks
- Filter sidebar allows re-querying with refined criteria

---

## ğŸ“¦ Project Structure


chatbot-theme-identifier/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”‚ â”œâ”€â”€ query.py â† Main QA logic
â”‚ â”‚ â”‚ â”œâ”€â”€ query_filters.py â† Filtering logic
â”‚ â”‚ â”‚ â””â”€â”€ documents.py â† List uploaded docs
â”‚ â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”‚ â”œâ”€â”€ text_extractor.py
â”‚ â”‚ â”‚ â””â”€â”€ embedding_pipeline.py
â”‚ â”‚ â””â”€â”€ main.py â† FastAPI entrypoint
â”œâ”€â”€ frontend/ (Next.js 14)
â”‚ â””â”€â”€ app/index/page.tsx â† Core Chat UI
â”œâ”€â”€ data/ â† Optional: test documents
â””â”€â”€ README.md


---

## ğŸš€ Running Locally

### 1ï¸âƒ£ Clone & Install

```bash
git clone https://github.com/yourname/document-theme-chatbot
cd document-theme-chatbot/backend
pip install -r requirements.txt

2ï¸âƒ£ Start Backend
# Start Qdrant (requires Docker)
docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant

# Start FastAPI server
uvicorn app.main:app --reload


3ï¸âƒ£ Start Frontend
cd ../document-theme-chatbot
npm install
npm run dev

App runs at http://localhost:3000 and connects to backend on http://localhost:8000

âœ… Setup via Docker 
To run Qdrant locally without manual installation, use the included docker-compose.yml.

1ï¸âƒ£ Add docker-compose.yml at the project root:
yaml
Copy
Edit
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_storage:/qdrant/storage
This will:

Pull the official Qdrant image

Expose it on localhost:6333

Mount a local qdrant_storage/ directory to persist vector data

2ï¸âƒ£ Create storage folder (optional)
bash
Copy
Edit
mkdir qdrant_storage
3ï¸âƒ£ Start Qdrant service
bash
Copy
Edit
docker-compose up -d
4ï¸âƒ£ Confirm itâ€™s running
Visit: http://localhost:6333

Your FastAPI backend connects automatically using:

python
Copy
Edit
QdrantClient(host="localhost", port=6333)


ğŸ¥ Demo Preview
ğŸ“ Upload documents
Drag and drop PDFs or images (OCR supported)

ğŸ’¬ Ask a question
Example: "What is the penalty mentioned in each compliance file?"

ğŸ“Š See answers:
Extracted text with DOC ID, Page, Chunk

Synthesized themes grouped across documents

Filter results by type/date/exclusion

Click chunk references to read source
